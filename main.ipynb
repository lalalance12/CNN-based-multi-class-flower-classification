{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CNN-based Multi-Class Flower Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Problem\n",
    "The goal of this project is to develop a Convolutional Neural Network (CNN) model that can accurately classify images of flowers into one of five categories: Sunflower, Orchid, Lotus, Lily, and Tulip. Accurate classification of flower species has applications in agriculture, botanical research, education and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition\n",
    "\n",
    "The dataset used in this project consists of images of five different flower species. It was obtained from kaggle.com where it was uploaded by Kausthub Kannan(owner) and edited/updated by DesolationOfSmaug(collaborator) (https://www.kaggle.com/datasets/kausthubkannan/5-flower-types-classification-dataset/data). The images are organized into folders named after each flower class. \n",
    "\n",
    "To prepare the data for modeling, the dataset will be split into training and testing sets with an 80-20 ratio. This ensures that the model can be trained on a substantial portion of the data while still being evaluated on unseen images. This is the link for the dataset that has already been split to training and testing - https://drive.google.com/drive/folders/1jiiJCfZIb_o87rKZtW6WBA99bzzuI1Cq?usp=sharing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note!*** \n",
    "\n",
    "Building and training this model was done in google colab. There could be minor errors you encounter if you try to run it on other platforms. It will be mostly problems relating to the directory of the dataset. All the necessaray information has been provided just change the code for the model to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "##### Splitting the Dataset into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path and output directory\n",
    "dataset_dir = '/content/drive/MyDrive/flower_images'  # Original dataset path in Google Drive\n",
    "output_dir = '/content/drive/MyDrive/flower_data_split'  # Output directory in MyDrive\n",
    "\n",
    "# Check if the output directory already exists to avoid re-splitting\n",
    "if not os.path.exists(output_dir):\n",
    "    import splitfolders  # Ensure you have splitfolders installed\n",
    "    # Split the dataset into training (80%) and testing (20%)\n",
    "    splitfolders.ratio(\n",
    "        dataset_dir,  # Path to the original dataset\n",
    "        output=output_dir,  # Output directory\n",
    "        seed=42,  # For reproducibility\n",
    "        ratio=(0.8, 0.2),  # 80% training, 20% testing\n",
    "    )\n",
    "\n",
    "# Define paths for training and testing directories\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "print(f\"Training data directory: {train_dir}\")\n",
    "print(f\"Testing data directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and Analysis\n",
    "\n",
    "##### Counting the Number of Images per Class\n",
    "We will analyze the distribution of images across the different classes in both the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pictures(folder_path):\n",
    "    \"\"\"Counts the number of image files in a folder.\"\"\"\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp')\n",
    "    image_count = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(image_extensions):\n",
    "            image_count += 1\n",
    "    return image_count\n",
    "\n",
    "# Define the main subdirectories\n",
    "main_subdirs = ['train', 'test']\n",
    "\n",
    "# Loop through the main subdirectories\n",
    "for main_subdir in main_subdirs:\n",
    "    main_subdir_path = os.path.join(output_dir, main_subdir)\n",
    "    # Get the subfolders within the main subdirectory\n",
    "    subfolders = [f for f in os.listdir(main_subdir_path) if os.path.isdir(os.path.join(main_subdir_path, f))]\n",
    "    # Loop through the subfolders and count pictures\n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(main_subdir_path, subfolder)\n",
    "        num_pictures = count_pictures(folder_path)\n",
    "        print(f\"Number of pictures in '{folder_path}': {num_pictures}\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "output_dir = '/content/drive/MyDrive/flower_data_split'\n",
    "\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "test_dir = os.path.join(output_dir, 'test')\n",
    "\n",
    "print(\"Train - Classes in the dataset:\")\n",
    "for folder in os.listdir(train_dir):\n",
    "    print(f\"- {folder}: {len(os.listdir(os.path.join(train_dir, folder)))} images\")\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(\"Test - Classes in the dataset:\")\n",
    "for folder in os.listdir(test_dir):\n",
    "    print(f\"- {folder}: {len(os.listdir(os.path.join(test_dir, folder)))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect class counts\n",
    "train_counts = {}\n",
    "test_counts = {}\n",
    "\n",
    "for folder in os.listdir(train_dir):\n",
    "    train_counts[folder] = len(os.listdir(os.path.join(train_dir, folder)))\n",
    "\n",
    "for folder in os.listdir(test_dir):\n",
    "    test_counts[folder] = len(os.listdir(os.path.join(test_dir, folder)))\n",
    "\n",
    "# Create DataFrames\n",
    "train_df = pd.DataFrame(list(train_counts.items()), columns=['Class', 'Count'])\n",
    "test_df = pd.DataFrame(list(test_counts.items()), columns=['Class', 'Count'])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='Class', y='Count', data=train_df)\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='Class', y='Count', data=test_df)\n",
    "plt.title('Testing Set Class Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation \n",
    "##### Data Augmentation and Preprocessing\n",
    "To enhance the model's ability to generalize, we will apply data augmentation techniques such as rotation, width and height shifts, shear, zoom, and horizontal flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescaling for test data\n",
    "\n",
    "# Creating the data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),  # Resizing images\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing Sample Augmented Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some images from the training set\n",
    "data_batch, labels_batch = next(train_generator)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img = data_batch[i]\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN Model\n",
    "\n",
    "We will construct a CNN model with convolutional layers followed by pooling layers, and a fully connected layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "We will compile the model using the Adam optimizer and categorical cross-entropy loss function, suitable for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We will train the model for up to 500 epochs with early stopping and learning rate reduction callbacks to prevent overfitting and optimize training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=500,\n",
    "    validation_data=test_generator,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** Training for 500 epochs may take a significant amount of time and could lead to overfitting. The EarlyStopping callback will halt training when the model stops improving on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "#### Plotting Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Metrics\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "model.save('/content/drive/MyDrive/flower_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Classification Report and Confusion Matrix\n",
    "To further assess the model's performance, we will generate a classification report and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Classification Report and Confusion Matrix\n",
    "test_generator.reset()\n",
    "Y_pred = model.predict(test_generator)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys()))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=test_generator.class_indices.keys(),\n",
    "            yticklabels=test_generator.class_indices.keys())\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a single image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(150, 150))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "    return img_array\n",
    "\n",
    "def make_prediction(image_path):\n",
    "    img_array = load_and_preprocess_image(image_path)\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    class_indices = {v: k for k, v in test_generator.class_indices.items()}\n",
    "    predicted_label = class_indices[predicted_class[0]]\n",
    "    return predicted_label\n",
    "\n",
    "image_path = 'Drive/sample_flower.jpg'\n",
    "predicted_label = make_prediction(image_path)\n",
    "print(f'The predicted class for the image is: {predicted_label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
